{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "    # \"\"\"Loads and preprocesses the dataset.\"\"\"\n",
    "df = pd.read_csv(\"D:\\\\All Data till 21st Feb 2025\\\\D-MART ANALYSIS\\\\fd_tickets_202502181901.csv\")\n",
    "df.columns = df.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "status\n",
       "resolved    95274\n",
       "closed      15128\n",
       "open         3450\n",
       "Resolved      942\n",
       "Closed        657\n",
       "pending        51\n",
       "New             2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"status\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akanksha.yadav\\AppData\\Local\\Temp\\ipykernel_15560\\1591448785.py:19: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\akanksha.yadav\\AppData\\Local\\Temp\\ipykernel_15560\\1591448785.py:29: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Unknown' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna(\"Unknown\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Only one class present in dataset. Cannot train model.\n",
      "Suggestion: Ensure the dataset contains both 'open' and 'closed' statuses.\n",
      "Skipping training due to insufficient data.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'drop'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 113\u001b[0m\n\u001b[0;32m    110\u001b[0m     exit()\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# Example prediction (for the first row of the dataset)\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m sample_data \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39miloc[:\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    114\u001b[0m sample_data \u001b[38;5;241m=\u001b[39m sample_data\u001b[38;5;241m.\u001b[39mapply(pd\u001b[38;5;241m.\u001b[39mto_numeric, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    115\u001b[0m sample_data\u001b[38;5;241m.\u001b[39mfillna(sample_data\u001b[38;5;241m.\u001b[39mmedian(), inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'drop'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"Loads and preprocesses the dataset.\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    date_columns = [\"First Trx Date\", \"Last Trx Date\", \"created_at\"]\n",
    "    for col in date_columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "    # Normalize status labels\n",
    "    df[\"status\"] = df[\"status\"].str.lower()\n",
    "    \n",
    "    # Drop columns with excessive missing values\n",
    "    drop_cols = [\"Chain\", \"Store Address\", \"Software Version\", \"First Trx Date\", \"Last Trx Date\"]\n",
    "    df.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "    \n",
    "    # Handle missing values\n",
    "    df.fillna(\"Unknown\", inplace=True)\n",
    "    \n",
    "    # Encode categorical variables before resampling\n",
    "    label_encoders = {}\n",
    "    categorical_cols = [\"Store\", \"State\", \"City\", \"Pin Code\", \"Terminal Category\", \"issue_type\", \"cf_issue_sub_category\", \"status\"]\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "    \n",
    "    # Convert all columns to numeric explicitly\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Fill missing values with median for numerical columns\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    \n",
    "    # Define target variable (predicting 'open' vs. 'closed')\n",
    "    df[\"Target\"] = df[\"status\"].apply(lambda x: 1 if x == \"open\" else 0)\n",
    "    \n",
    "    # Check if dataset contains only one class\n",
    "    if df[\"Target\"].nunique() < 2:\n",
    "        print(\"Error: Only one class present in dataset. Cannot train model.\")\n",
    "        print(\"Suggestion: Ensure the dataset contains both 'open' and 'closed' statuses.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    return df, label_encoders, df[\"Target\"].nunique()\n",
    "\n",
    "def train_model(df):\n",
    "    \"\"\"Trains a RandomForest model with stratified cross-validation and undersampling to balance classes.\"\"\"\n",
    "    if df is None:\n",
    "        print(\"Skipping training due to insufficient data.\")\n",
    "        return None, None\n",
    "    \n",
    "    X = df.drop(columns=[\"Target\"])\n",
    "    y = df[\"Target\"]\n",
    "    \n",
    "    # Ensure class balance in splitting\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Apply Random Undersampling to balance classes\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "    print(f\"Training class distribution after undersampling: {np.bincount(y_train)}\")\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        max_depth=10, \n",
    "        min_samples_split=10, \n",
    "        min_samples_leaf=5, \n",
    "        max_features='sqrt', \n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=skf, scoring='roc_auc')\n",
    "    print(f\"Cross-Validation ROC-AUC: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")\n",
    "    \n",
    "    # Save the model and scaler\n",
    "    joblib.dump(model, \"dmart_issue_predictor.pkl\")\n",
    "    joblib.dump(scaler, \"scaler.pkl\")\n",
    "    \n",
    "    print(\"Model training complete.\")\n",
    "    return model, scaler\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess the data\n",
    "    file_path = \"fd_tickets_202502181901.csv\"\n",
    "    df, label_encoders, class_count = load_and_preprocess_data(file_path)\n",
    "    \n",
    "    if df is None:\n",
    "        exit()\n",
    "    \n",
    "    # Train the model\n",
    "    model, scaler = train_model(df)\n",
    "    if model is None:\n",
    "        exit()\n",
    "    \n",
    "    # Example prediction (for the first row of the dataset)\n",
    "    sample_data = df.drop(columns=[\"Target\"]).iloc[:1]\n",
    "    sample_data = sample_data.apply(pd.to_numeric, errors='coerce')\n",
    "    sample_data.fillna(sample_data.median(), inplace=True)\n",
    "    \n",
    "    if model and scaler:\n",
    "        predicted_issue = predict_issues(model, scaler, sample_data)\n",
    "        issue_type = label_encoders[\"issue_type\"].inverse_transform([predicted_issue[0]])[0]\n",
    "        print(f\"Predicted Issue Type: {issue_type}\")\n",
    "    else:\n",
    "        print(\"Skipping prediction due to failed training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"Loads and preprocesses the dataset.\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    date_columns = [\"First Trx Date\", \"Last Trx Date\", \"created_at\"]\n",
    "    for col in date_columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "    # Normalize status labels\n",
    "    df[\"status\"] = df[\"status\"].str.lower()\n",
    "    \n",
    "    # Drop columns with excessive missing values\n",
    "    drop_cols = [\"Chain\", \"Store Address\", \"Software Version\", \"First Trx Date\", \"Last Trx Date\"]\n",
    "    df.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "    \n",
    "    # Handle missing values\n",
    "    df.fillna(\"Unknown\", inplace=True)\n",
    "    \n",
    "    # Encode categorical variables before resampling\n",
    "    label_encoders = {}\n",
    "    categorical_cols = [\"Store\", \"State\", \"City\", \"Pin Code\", \"Terminal Category\", \"issue_type\", \"cf_issue_sub_category\", \"status\"]\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "    \n",
    "    # Convert all columns to numeric explicitly\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Fill missing values with median for numerical columns\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    \n",
    "    # Define target variable (predicting 'open' vs. 'closed')\n",
    "    df[\"Target\"] = df[\"status\"].apply(lambda x: 1 if x == \"open\" else 0)\n",
    "    \n",
    "    # Check if dataset contains only one class\n",
    "    if df[\"Target\"].nunique() < 2:\n",
    "        print(\"Error: Only one class present in dataset. Cannot train model.\")\n",
    "        print(\"Suggestion: Ensure the dataset contains both 'open' and 'closed' statuses.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    return df, label_encoders, df[\"Target\"].nunique()\n",
    "\n",
    "def train_model(df):\n",
    "    \"\"\"Trains a RandomForest model with stratified cross-validation and undersampling to balance classes.\"\"\"\n",
    "    if df is None:\n",
    "        print(\"Skipping training due to insufficient data.\")\n",
    "        return None, None\n",
    "    \n",
    "    X = df.drop(columns=[\"Target\"])\n",
    "    y = df[\"Target\"]\n",
    "    \n",
    "    # Ensure class balance in splitting\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "#     # Apply Random Undersampling to balance classes\n",
    "#     rus = RandomUnderSampler(random_state=42)\n",
    "#     X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "#     print(f\"Training class distribution after undersampling: {np.bincount(y_train)}\")\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     X_train_scaled = scaler.fit_transform(X_train)\n",
    "#     X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "#     model = RandomForestClassifier(\n",
    "#         n_estimators=100, \n",
    "#         max_depth=10, \n",
    "#         min_samples_split=10, \n",
    "#         min_samples_leaf=5, \n",
    "#         max_features='sqrt', \n",
    "#         random_state=42\n",
    "#     )\n",
    "#     model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "#     # Cross-validation\n",
    "#     skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#     cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=skf, scoring='roc_auc')\n",
    "#     print(f\"Cross-Validation ROC-AUC: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")\n",
    "    \n",
    "#     # Save the model and scaler\n",
    "#     joblib.dump(model, \"dmart_issue_predictor.pkl\")\n",
    "#     joblib.dump(scaler, \"scaler.pkl\")\n",
    "    \n",
    "#     print(\"Model training complete.\")\n",
    "#     return model, scaler\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Load and preprocess the data\n",
    "#     file_path = \"fd_tickets_202502181901.csv\"\n",
    "#     df, label_encoders, class_count = load_and_preprocess_data(file_path)\n",
    "    \n",
    "#     if df is None:\n",
    "#         exit()\n",
    "    \n",
    "#     # Train the model\n",
    "#     model, scaler = train_model(df)\n",
    "#     if model is None:\n",
    "#         exit()\n",
    "    \n",
    "#     # Example prediction (for the first row of the dataset)\n",
    "#     sample_data = df.drop(columns=[\"Target\"]).iloc[:1]\n",
    "#     sample_data = sample_data.apply(pd.to_numeric, errors='coerce')\n",
    "#     sample_data.fillna(sample_data.median(), inplace=True)\n",
    "    \n",
    "#     # if model and scaler:\n",
    "#         predicted_issue = predict_issues(model, scaler, sample_data)\n",
    "#         issue_type = label_encoders[\"issue_type\"].inverse_transform([predicted_issue[0]])[0]\n",
    "#         print(f\"Predicted Issue Type: {issue_type}\")\n",
    "#     else:\n",
    "#         print(\"Skipping prediction due to failed training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akanksha.yadav\\AppData\\Local\\Temp\\ipykernel_63692\\3544930658.py:19: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\akanksha.yadav\\AppData\\Local\\Temp\\ipykernel_63692\\3544930658.py:29: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Unknown' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna(\"Unknown\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Only one class present in dataset. Cannot train model.\n",
      "Suggestion: Ensure the dataset contains both 'open' and 'closed' statuses.\n",
      "Class distribution after preprocessing:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 124\u001b[0m\n\u001b[0;32m    122\u001b[0m df, label_encoders, class_count \u001b[38;5;241m=\u001b[39m load_and_preprocess_data(file_path)\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass distribution after preprocessing:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 124\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTarget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalue_counts())\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# if df is None:\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m#     exit()\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"Loads and preprocesses the dataset.\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    date_columns = [\"First Trx Date\", \"Last Trx Date\", \"created_at\"]\n",
    "    for col in date_columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "    # Normalize status labels\n",
    "    df[\"status\"] = df[\"status\"].str.lower()\n",
    "    \n",
    "    # Drop columns with excessive missing values\n",
    "    drop_cols = [\"Chain\", \"Store Address\", \"Software Version\", \"First Trx Date\", \"Last Trx Date\"]\n",
    "    df.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "    \n",
    "    # Handle missing values\n",
    "    df.fillna(\"Unknown\", inplace=True)\n",
    "    \n",
    "    # Encode categorical variables before resampling\n",
    "    label_encoders = {}\n",
    "    categorical_cols = [\"Store\", \"State\", \"City\", \"Pin Code\", \"Terminal Category\", \"issue_type\", \"cf_issue_sub_category\", \"status\"]\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "    \n",
    "    # Convert all columns to numeric explicitly\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Fill missing values with median for numerical columns\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    \n",
    "    # Define target variable (predicting 'open' vs. 'closed')\n",
    "    # df[\"Target\"] = df[\"status\"].apply(lambda x: 1 if x == \"open\" else 0)\n",
    "    # df[\"Target\"] = df[\"status\"].apply(lambda x: 1 if \"open\" in x.lower() else 0)\n",
    "    df[\"status\"] = df[\"status\"].astype(str)  # Convert entire column to string first\n",
    "    df[\"Target\"] = df[\"status\"].apply(lambda x: 1 if \"open\" in x.lower() else 0)\n",
    "\n",
    "\n",
    "    # Check if dataset contains only one class\n",
    "    if df[\"Target\"].nunique() < 2:\n",
    "        print(\"Error: Only one class present in dataset. Cannot train model.\")\n",
    "        print(\"Suggestion: Ensure the dataset contains both 'open' and 'closed' statuses.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    return df, label_encoders, df[\"Target\"].nunique()\n",
    "\n",
    "def train_model(df):\n",
    "    \"\"\"Trains a RandomForest model with stratified cross-validation and undersampling to balance classes.\"\"\"\n",
    "    if df is None:\n",
    "        print(\"Skipping training due to insufficient data.\")\n",
    "        return None, None\n",
    "    \n",
    "    X = df.drop(columns=[\"Target\"])\n",
    "    y = df[\"Target\"]\n",
    "    \n",
    "    # Ensure class balance in splitting\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Apply Random Undersampling to balance classes\n",
    "    # rus = RandomUnderSampler(random_state=42)\n",
    "    # rus = RandomUnderSampler(sampling_strategy=0.5, random_state=42)\n",
    "    # rus = RandomUnderSampler(sampling_strategy={0: 2 * df[\"Target\"].sum(), 1: df[\"Target\"].sum()}, random_state=42)\n",
    "\n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    ros = RandomOverSampler(sampling_strategy=0.5, random_state=42)\n",
    "    X_train, y_train = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "    X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "    print(f\"Training class distribution after undersampling: {np.bincount(y_train)}\")\n",
    "    \n",
    "    if df is None or model is None:\n",
    "     print(\"Skipping prediction due to insufficient data.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        max_depth=10, \n",
    "        min_samples_split=10, \n",
    "        min_samples_leaf=5, \n",
    "        max_features='sqrt', \n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=skf, scoring='roc_auc')\n",
    "    print(f\"Cross-Validation ROC-AUC: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")\n",
    "    \n",
    "    # Save the model and scaler\n",
    "    joblib.dump(model, \"dmart_issue_predictor.pkl\")\n",
    "    joblib.dump(scaler, \"scaler.pkl\")\n",
    "    \n",
    "    print(\"Model training complete.\")\n",
    "    return model, scaler\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess the data\n",
    "    file_path = \"fd_tickets_202502181901.csv\"\n",
    "    df, label_encoders, class_count = load_and_preprocess_data(file_path)\n",
    "    print(\"Class distribution after preprocessing:\")\n",
    "    print(df[\"Target\"].value_counts())\n",
    "\n",
    "    \n",
    "    # if df is None:\n",
    "    #     exit()\n",
    "    if df is None:\n",
    "        print(\"Error: Dataset is empty or contains only one class. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    # Train the model\n",
    "    model, scaler = train_model(df)\n",
    "    if model is None:\n",
    "        exit()\n",
    "    \n",
    "    # Example prediction (for the first row of the dataset)\n",
    "    sample_data = df.drop(columns=[\"Target\"]).iloc[:1]\n",
    "    sample_data = sample_data.apply(pd.to_numeric, errors='coerce')\n",
    "    sample_data.fillna(sample_data.median(), inplace=True)\n",
    "    \n",
    "    if model and scaler:\n",
    "        predicted_issue = predict_issues(model, scaler, sample_data)\n",
    "        issue_type = label_encoders[\"issue_type\"].inverse_transform([predicted_issue[0]])[0]\n",
    "        print(f\"Predicted Issue Type: {issue_type}\")\n",
    "    else:\n",
    "        print(\"Skipping prediction due to failed training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akanksha.yadav\\AppData\\Local\\Temp\\ipykernel_63692\\153348212.py:18: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\akanksha.yadav\\AppData\\Local\\Temp\\ipykernel_63692\\153348212.py:25: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Unknown' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna(\"Unknown\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation R2 Score: 0.8624 (+/- 0.0119)\n",
      "Model training complete.\n",
      "Predicted Number of Issues for Store: 265\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"Loads and preprocesses the dataset without using the status column.\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    date_columns = [\"First Trx Date\", \"Last Trx Date\", \"created_at\"]\n",
    "    for col in date_columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    drop_cols = [\"Chain\", \"Store Address\", \"Software Version\", \"First Trx Date\", \"Last Trx Date\", \"status\"]\n",
    "    df.drop(columns=[col for col in drop_cols if col in df.columns], inplace=True, errors='ignore')\n",
    "    \n",
    "    # Handle missing values\n",
    "    df.fillna(\"Unknown\", inplace=True)\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    label_encoders = {}\n",
    "    categorical_cols = [\"Store\", \"State\", \"City\", \"Pin Code\", \"Terminal Category\", \"issue_type\", \"cf_issue_sub_category\"]\n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            le = LabelEncoder()\n",
    "            df[col] = le.fit_transform(df[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    # Convert all columns to numeric explicitly\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Fill missing values with median for numerical columns\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    \n",
    "    # Define target variable (predicting issue frequency per store)\n",
    "    df[\"Target\"] = df.groupby(\"Store\")[\"ticket_id\"].transform(\"count\")\n",
    "    \n",
    "    return df, label_encoders\n",
    "\n",
    "def train_model(df):\n",
    "    \"\"\"Trains a RandomForest model to predict issue frequency per store.\"\"\"\n",
    "    X = df.drop(columns=[\"Target\"])\n",
    "    y = df[\"Target\"]\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        max_depth=10, \n",
    "        min_samples_split=10, \n",
    "        min_samples_leaf=5, \n",
    "        max_features='sqrt', \n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=skf, scoring='r2')\n",
    "    print(f\"Cross-Validation R2 Score: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")\n",
    "    \n",
    "    # Save the model and scaler\n",
    "    joblib.dump(model, \"dmart_issue_predictor.pkl\")\n",
    "    joblib.dump(scaler, \"scaler.pkl\")\n",
    "    \n",
    "    print(\"Model training complete.\")\n",
    "    return model, scaler\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess the data\n",
    "    file_path = \"fd_tickets_202502181901.csv\"\n",
    "    df, label_encoders = load_and_preprocess_data(file_path)\n",
    "    \n",
    "    # Train the model\n",
    "    model, scaler = train_model(df)\n",
    "    \n",
    "    # Example prediction (for the first row of the dataset)\n",
    "    sample_data = df.drop(columns=[\"Target\"]).iloc[:1]\n",
    "    sample_data = sample_data.apply(pd.to_numeric, errors='coerce')\n",
    "    sample_data.fillna(sample_data.median(), inplace=True)\n",
    "    \n",
    "    if model and scaler:\n",
    "        predicted_issue_count = model.predict(scaler.transform(sample_data))[0]\n",
    "        print(f\"Predicted Number of Issues for Store: {predicted_issue_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting uvicorn\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\akanksha.yadav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from uvicorn) (8.1.8)\n",
      "Collecting h11>=0.8 (from uvicorn)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\akanksha.yadav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click>=7.0->uvicorn) (0.4.6)\n",
      "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "   ---------------------------------------- 0.0/62.3 kB ? eta -:--:--\n",
      "   -------------------------------- ------- 51.2/62.3 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 62.3/62.3 kB 1.7 MB/s eta 0:00:00\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.3/58.3 kB 3.0 MB/s eta 0:00:00\n",
      "Installing collected packages: h11, uvicorn\n",
      "Successfully installed h11-0.14.0 uvicorn-0.34.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\akanksha.yadav\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install uvicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akanksha.yadav\\AppData\\Local\\Temp\\ipykernel_63692\\1555256078.py:19: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\akanksha.yadav\\AppData\\Local\\Temp\\ipykernel_63692\\1555256078.py:26: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Unknown' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna(\"Unknown\", inplace=True)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 126\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     model, scaler \u001b[38;5;241m=\u001b[39m train_model(df)\n\u001b[1;32m--> 126\u001b[0m \u001b[43muvicorn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0.0.0.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\uvicorn\\main.py:579\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(app, host, port, uds, fd, loop, http, ws, ws_max_size, ws_max_queue, ws_ping_interval, ws_ping_timeout, ws_per_message_deflate, lifespan, interface, reload, reload_dirs, reload_includes, reload_excludes, reload_delay, workers, env_file, log_config, log_level, access_log, proxy_headers, server_header, date_header, forwarded_allow_ips, root_path, limit_concurrency, backlog, limit_max_requests, timeout_keep_alive, timeout_graceful_shutdown, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_version, ssl_cert_reqs, ssl_ca_certs, ssl_ciphers, headers, use_colors, app_dir, factory, h11_max_incomplete_event_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m         Multiprocess(config, target\u001b[38;5;241m=\u001b[39mserver\u001b[38;5;241m.\u001b[39mrun, sockets\u001b[38;5;241m=\u001b[39m[sock])\u001b[38;5;241m.\u001b[39mrun()\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 579\u001b[0m         \u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# pragma: full coverage\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\uvicorn\\server.py:66\u001b[0m, in \u001b[0;36mServer.run\u001b[1;34m(self, sockets)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, sockets: \u001b[38;5;28mlist\u001b[39m[socket\u001b[38;5;241m.\u001b[39msocket] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39msetup_event_loop()\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43msockets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msockets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\runners.py:186\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "from fastapi import FastAPI\n",
    "import uvicorn\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_and_preprocess_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    date_columns = [\"First Trx Date\", \"Last Trx Date\", \"created_at\"]\n",
    "    for col in date_columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    drop_cols = [\"Chain\", \"Store Address\", \"Software Version\", \"First Trx Date\", \"Last Trx Date\", \"status\"]\n",
    "    df.drop(columns=[col for col in drop_cols if col in df.columns], inplace=True, errors='ignore')\n",
    "    \n",
    "    # Handle missing values\n",
    "    df.fillna(\"Unknown\", inplace=True)\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    label_encoders = {}\n",
    "    categorical_cols = [\"Store\", \"State\", \"City\", \"Pin Code\", \"Terminal Category\", \"issue_type\", \"cf_issue_sub_category\"]\n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            le = LabelEncoder()\n",
    "            df[col] = le.fit_transform(df[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    # Convert all columns to numeric explicitly\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Fill missing values with median for numerical columns\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    \n",
    "    # Define target variable (predicting issue frequency per store)\n",
    "    df[\"Target\"] = df.groupby(\"Store\")[\"ticket_id\"].transform(\"count\")\n",
    "    \n",
    "    return df, label_encoders\n",
    "\n",
    "# Train the model\n",
    "def train_model(df):\n",
    "    X = df.drop(columns=[\"Target\"])\n",
    "    y = df[\"Target\"]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        max_depth=10, \n",
    "        min_samples_split=10, \n",
    "        min_samples_leaf=5, \n",
    "        max_features='sqrt', \n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Save the model and scaler\n",
    "    joblib.dump(model, \"dmart_issue_predictor.pkl\")\n",
    "    joblib.dump(scaler, \"scaler.pkl\")\n",
    "    \n",
    "    print(\"Model training complete.\")\n",
    "    return model, scaler\n",
    "\n",
    "# Load trained model\n",
    "try:\n",
    "    model = joblib.load(\"dmart_issue_predictor.pkl\")\n",
    "    scaler = joblib.load(\"scaler.pkl\")\n",
    "except:\n",
    "    model, scaler = None, None\n",
    "\n",
    "# FastAPI chatbot application\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"message\": \"Welcome to the D-Mart Operations Chatbot! Ask anything about D-Mart issues.\"}\n",
    "\n",
    "@app.get(\"/query\")\n",
    "def query_data(query: str):\n",
    "    if \"highest issues\" in query.lower():\n",
    "        top_store = df.groupby(\"Store\")[\"Target\"].sum().idxmax()\n",
    "        return {\"response\": f\"The store with the highest number of issues is {top_store}.\"}\n",
    "    elif \"issues in last month\" in query.lower():\n",
    "        last_month = df[df[\"created_at\"] >= pd.to_datetime(\"now\") - pd.DateOffset(months=1)]\n",
    "        return {\"response\": f\"There were {len(last_month)} issues reported in the last month.\"}\n",
    "    elif \"store\" in query.lower():\n",
    "        store_name = query.split(\"store\")[-1].strip()\n",
    "        store_issues = df[df[\"Store\"] == store_name]\n",
    "        return {\"response\": f\"Store {store_name} has {len(store_issues)} reported issues.\"}\n",
    "    else:\n",
    "        return {\"response\": \"I couldn't understand your question. Please ask about D-Mart operations!\"}\n",
    "\n",
    "@app.get(\"/predict\")\n",
    "def predict_issues(store_id: int):\n",
    "    if model is None or scaler is None:\n",
    "        return {\"error\": \"Model not trained yet!\"}\n",
    "    \n",
    "    store_data = df[df[\"Store\"] == store_id].drop(columns=[\"Target\"]).iloc[:1]\n",
    "    if store_data.empty:\n",
    "        return {\"error\": \"Store ID not found in dataset!\"}\n",
    "    \n",
    "    store_data = store_data.apply(pd.to_numeric, errors='coerce')\n",
    "    store_data.fillna(store_data.median(), inplace=True)\n",
    "    predicted_issues = model.predict(scaler.transform(store_data))[0]\n",
    "    \n",
    "    return {\"store_id\": store_id, \"predicted_issues\": int(predicted_issues)}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"fd_tickets_202502181901.csv\"\n",
    "    df, label_encoders = load_and_preprocess_data(file_path)\n",
    "    if model is None:\n",
    "        model, scaler = train_model(df)\n",
    "    \n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akanksha.yadav\\AppData\\Local\\Temp\\ipykernel_63692\\1475795404.py:19: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df[col] = pd.to_datetime(df[col], errors='coerce')\n",
      "C:\\Users\\akanksha.yadav\\AppData\\Local\\Temp\\ipykernel_63692\\1475795404.py:26: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Unknown' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna(\"Unknown\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inside an active event loop. Use `!uvicorn your_script:app --reload` in Jupyter Notebook or VS Code terminal.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "from fastapi import FastAPI\n",
    "import uvicorn\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_and_preprocess_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    date_columns = [\"First Trx Date\", \"Last Trx Date\", \"created_at\"]\n",
    "    for col in date_columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    drop_cols = [\"Chain\", \"Store Address\", \"Software Version\", \"First Trx Date\", \"Last Trx Date\", \"status\"]\n",
    "    df.drop(columns=[col for col in drop_cols if col in df.columns], inplace=True, errors='ignore')\n",
    "    \n",
    "    # Handle missing values\n",
    "    df.fillna(\"Unknown\", inplace=True)\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    label_encoders = {}\n",
    "    categorical_cols = [\"Store\", \"State\", \"City\", \"Pin Code\", \"Terminal Category\", \"issue_type\", \"cf_issue_sub_category\"]\n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            le = LabelEncoder()\n",
    "            df[col] = le.fit_transform(df[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    # Convert all columns to numeric explicitly\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Fill missing values with median for numerical columns\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    \n",
    "    # Define target variable (predicting issue frequency per store)\n",
    "    df[\"Target\"] = df.groupby(\"Store\")[\"ticket_id\"].transform(\"count\")\n",
    "    \n",
    "    return df, label_encoders\n",
    "\n",
    "# Train the model\n",
    "def train_model(df):\n",
    "    X = df.drop(columns=[\"Target\"])\n",
    "    y = df[\"Target\"]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        max_depth=10, \n",
    "        min_samples_split=10, \n",
    "        min_samples_leaf=5, \n",
    "        max_features='sqrt', \n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Save the model and scaler\n",
    "    joblib.dump(model, \"dmart_issue_predictor.pkl\")\n",
    "    joblib.dump(scaler, \"scaler.pkl\")\n",
    "    \n",
    "    print(\"Model training complete.\")\n",
    "    return model, scaler\n",
    "\n",
    "# Load trained model\n",
    "try:\n",
    "    model = joblib.load(\"dmart_issue_predictor.pkl\")\n",
    "    scaler = joblib.load(\"scaler.pkl\")\n",
    "except:\n",
    "    model, scaler = None, None\n",
    "\n",
    "# FastAPI chatbot application\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"message\": \"Welcome to the D-Mart Operations Chatbot! Ask anything about D-Mart issues.\"}\n",
    "\n",
    "@app.get(\"/query\")\n",
    "def query_data(query: str):\n",
    "    if \"highest issues\" in query.lower():\n",
    "        top_store = df.groupby(\"Store\")[\"Target\"].sum().idxmax()\n",
    "        return {\"response\": f\"The store with the highest number of issues is {top_store}.\"}\n",
    "    elif \"issues in last month\" in query.lower():\n",
    "        last_month = df[df[\"created_at\"] >= pd.to_datetime(\"now\") - pd.DateOffset(months=1)]\n",
    "        return {\"response\": f\"There were {len(last_month)} issues reported in the last month.\"}\n",
    "    elif \"store\" in query.lower():\n",
    "        store_name = query.split(\"store\")[-1].strip()\n",
    "        store_issues = df[df[\"Store\"] == store_name]\n",
    "        return {\"response\": f\"Store {store_name} has {len(store_issues)} reported issues.\"}\n",
    "    else:\n",
    "        return {\"response\": \"I couldn't understand your question. Please ask about D-Mart operations!\"}\n",
    "\n",
    "@app.get(\"/predict\")\n",
    "def predict_issues(store_id: int):\n",
    "    if model is None or scaler is None:\n",
    "        return {\"error\": \"Model not trained yet!\"}\n",
    "    \n",
    "    store_data = df[df[\"Store\"] == store_id].drop(columns=[\"Target\"]).iloc[:1]\n",
    "    if store_data.empty:\n",
    "        return {\"error\": \"Store ID not found in dataset!\"}\n",
    "    \n",
    "    store_data = store_data.apply(pd.to_numeric, errors='coerce')\n",
    "    store_data.fillna(store_data.median(), inplace=True)\n",
    "    predicted_issues = model.predict(scaler.transform(store_data))[0]\n",
    "    \n",
    "    return {\"store_id\": store_id, \"predicted_issues\": int(predicted_issues)}\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     file_path = \"fd_tickets_202502181901.csv\"\n",
    "#     df, label_encoders = load_and_preprocess_data(file_path)\n",
    "#     if model is None:\n",
    "#         model, scaler = train_model(df)\n",
    "    \n",
    "#     uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "\n",
    "import asyncio\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"fd_tickets_202502181901.csv\"\n",
    "    df, label_encoders = load_and_preprocess_data(file_path)\n",
    "    if model is None:\n",
    "        model, scaler = train_model(df)\n",
    "    \n",
    "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000)\n",
    "    server = uvicorn.Server(config)\n",
    "\n",
    "    if asyncio.get_event_loop().is_running():\n",
    "        print(\"Running inside an active event loop. Use `!uvicorn your_script:app --reload` in Jupyter Notebook or VS Code terminal.\")\n",
    "    else:\n",
    "        server.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacyNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading spacy-3.8.4-cp311-cp311-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.12-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp311-cp311-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.4-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\akanksha.yadav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\akanksha.yadav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (2.2.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\akanksha.yadav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\akanksha.yadav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\akanksha.yadav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (3.1.5)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\akanksha.yadav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (24.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\akanksha.yadav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\akanksha.yadav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\akanksha.yadav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\akanksha.yadav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\akanksha.yadav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\akanksha.yadav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\akanksha.yadav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.2.0-cp311-cp311-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\akanksha.yadav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\akanksha.yadav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\akanksha.yadav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.20.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\akanksha.yadav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp311-cp311-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\akanksha.yadav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\akanksha.yadav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading wrapt-1.17.2-cp311-cp311-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\akanksha.yadav\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Downloading spacy-3.8.4-cp311-cp311-win_amd64.whl (12.2 MB)\n",
      "   ---------------------------------------- 0.0/12.2 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.0/12.2 MB 5.0 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.4/12.2 MB 5.8 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.7/12.2 MB 6.1 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 5.0/12.2 MB 6.0 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 6.0/12.2 MB 5.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 7.1/12.2 MB 5.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 8.1/12.2 MB 5.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.9/12.2 MB 5.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.0/12.2 MB 5.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.7/12.2 MB 5.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.8/12.2 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.2 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.2/12.2 MB 4.7 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.12-cp311-cp311-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp311-cp311-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 632.6/632.6 kB 3.0 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.4-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 0.8/1.5 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 3.7 MB/s eta 0:00:00\n",
      "Downloading typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading blis-1.2.0-cp311-cp311-win_amd64.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.8/6.2 MB 3.7 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.8/6.2 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 2.9/6.2 MB 4.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.7/6.2 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.5/6.2 MB 4.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.5/6.2 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 4.3 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.0/5.4 MB 4.6 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.1/5.4 MB 4.9 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.1/5.4 MB 5.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.5/5.4 MB 5.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 5.2/5.4 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 4.7 MB/s eta 0:00:00\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading marisa_trie-1.2.1-cp311-cp311-win_amd64.whl (152 kB)\n",
      "Downloading wrapt-1.17.2-cp311-cp311-win_amd64.whl (38 kB)\n",
      "Installing collected packages: cymem, wrapt, wasabi, spacy-loggers, spacy-legacy, shellingham, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, srsly, smart-open, preshed, language-data, typer, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-1.2.0 catalogue-2.0.10 cloudpathlib-0.20.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.12 preshed-3.0.9 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 typer-0.15.1 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.2\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"SpaCy model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
